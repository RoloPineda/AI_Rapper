{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# opening eminem lyrics data set\n",
    "def open_file(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Convert to lower case and save as a list\n",
    "        corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "    return corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "filename = \"eminem_Lyrics.txt\"\n",
    "corpus = open_file(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 507 lines of lyrics\n",
      "\n",
      "The first 5 lines look like this:\n",
      "\n",
      "now this shit's about to kick off this party looks wack\n",
      "let's take it back to straight hip-hop and start it from scratch\n",
      "i'm 'bout to bloody this track up, everybody get back\n",
      "that's why my pen needs a pad cause my rhymes on the rage\n",
      "just like i did with addiction i'm 'bout to kick it\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(corpus)} lines of lyrics\\n\")\n",
    "print(f\"The first 5 lines look like this:\\n\")\n",
    "for i in range(5):\n",
    "  print(corpus[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index dictionary: {'i': 1, 'a': 2, 'the': 3, 'you': 4, 'to': 5, 'and': 6, 'it': 7, \"i'm\": 8, 'my': 9, 'me': 10, 'in': 11, 'that': 12, 'like': 13, 'but': 14, 'your': 15, 'of': 16, 'doing': 17, 'get': 18, 'on': 19, 'so': 20, 'this': 21, 'with': 22, 'be': 23, \"don't\": 24, 'just': 25, 'go': 26, 'can': 27, 'pee': 28, 'as': 29, \"you're\": 30, 'back': 31, 'for': 32, 'make': 33, 'is': 34, 'got': 35, 'not': 36, 'know': 37, 'they': 38, \"it's\": 39, 'what': 40, 'way': 41, \"ain't\": 42, 'out': 43, 'say': 44, \"'cause\": 45, 'up': 46, 'rap': 47, 'was': 48, 'who': 49, 'never': 50, 'from': 51, 'when': 52, 'all': 53, 'at': 54, 'think': 55, 'now': 56, 'still': 57, 'down': 58, 'baby': 59, 'fuck': 60, 'if': 61, 'd': 62, 'are': 63, 'oh': 64, 'man': 65, 'see': 66, 'do': 67, 'off': 68, 'bitch': 69, 'girl': 70, 'have': 71, \"lookin'\": 72, 'boy': 73, 'everybody': 74, 'he': 75, 'about': 76, 'take': 77, \"'em\": 78, 'mean': 79, 'how': 80, 'feel': 81, 'am': 82, 'her': 83, 'really': 84, \"i'll\": 85, 'ass': 86, \"that's\": 87, 'made': 88, 'enough': 89, 'one': 90, \"fuckin'\": 91, 'put': 92, 'day': 93, 'give': 94, 'god': 95, 'seen': 96, \"let's\": 97, 'we': 98, 'love': 99, 'come': 100, 'no': 101, 'next': 102, 'want': 103, 'time': 104, \"he's\": 105, 'little': 106, 'an': 107, 'then': 108, 'please': 109, \"can't\": 110, 'why': 111, 'did': 112, 'slim': 113, 'let': 114, 'gonna': 115, 'rock': 116, 'before': 117, 'too': 118, 'or': 119, 'mouth': 120, 'woo': 121, 'had': 122, 'yeah': 123, 'look': 124, 'him': 125, 'even': 126, 'nod': 127, 'move': 128, 'freeze': 129, 'yourself': 130, 'shake': 131, 'into': 132, 'shit': 133, 'name': 134, 'old': 135, 'talk': 136, \"i'ma\": 137, 'only': 138, 'front': 139, 'gotta': 140, 'his': 141, 'big': 142, 'these': 143, 'will': 144, 'through': 145, 'blame': 146, 'she': 147, 'them': 148, 'while': 149, 'believe': 150, 'joke': 151, 'kick': 152, \"'bout\": 153, 'sick': 154, 'since': 155, 'long': 156, 'weird': 157, \"we're\": 158, \"'til\": 159, 'than': 160, 'yes': 161, 'sorry': 162, 'done': 163, 'though': 164, 'mathers': 165, 'damn': 166, 'call': 167, \"you'll\": 168, 'need': 169, 'year': 170, 'more': 171, 'again': 172, 'said': 173, 'six': 174, 'good': 175, 'doc': 176, 'jessica': 177, 'clear': 178, 'came': 179, 'wants': 180, 'any': 181, 'well': 182, 'people': 183, 'new': 184, 'j': 185, 'chong': 186, 'kid': 187, 'straight': 188, 'hip': 189, 'hop': 190, 'pad': 191, 'rhymes': 192, 'rage': 193, 'looking': 194, 'wait': 195, 'ya': 196, 'friend': 197, 'berzerk': 198, 'night': 199, 'grow': 200, 'broke': 201, 'guess': 202, 'same': 203, 'pow': 204, 'hard': 205, 'marshall': 206, 'head': 207, 'heart': 208, 'word': 209, 'fans': 210, 'mic': 211, 'once': 212, 'hit': 213, 'last': 214, 'line': 215, \"here's\": 216, 'gun': 217, 'kelly': 218, 'by': 219, 'three': 220, 'albums': 221, 'ho': 222, 'shoot': 223, 'own': 224, 'diddy': 225, 'blow': 226, 'some': 227, 'cock': 228, 'pop': 229, 'deep': 230, 'shady': 231, 'keep': 232, 'door': 233, 'fact': 234, 'women': 235, 'walk': 236, 'forget': 237, 'should': 238, 'here': 239, 'bake': 240, 'break': 241, 'minutes': 242, \"beginnin'\": 243, 'half': 244, \"motherfuckin'\": 245, 'school': 246, 'music': 247, 'hey': 248, 'fame': 249, 'gay': 250, 'face': 251, 'king': 252, 'away': 253, 'police': 254, 'saying': 255, 'mr': 256, 'officer': 257, 'triumph': 258, 'puppet': 259, 'free': 260, 'looks': 261, 'wack': 262, 'scratch': 263, 'track': 264, 'needs': 265, 'cause': 266, 'turn': 267, 'quick': 268, 'tell': 269, 'da': 270, 'mc': 271, 'been': 272, 'shoes': 273, 'hair': 274, 'beard': 275, 'knock': 276, 'm': 277, 'k': 278, 'fed': 279, \"life's\": 280, 'game': 281, 'mate': 282, 'dang': 283, 'baw': 284, 'chica': 285, 'wow': 286, 'gal': 287, 'throw': 288, 'huh': 289, 'least': 290, 'fell': 291, 'woke': 292, 'yo': 293, 'bar': 294, 'future': 295, 'money': 296, 'car': 297, 'unless': 298, 'jam': 299, 'haters': 300, \"beard's\": 301, \"yellin'\": 302, 'attack': 303, 'mile': 304, 'realized': 305, 'daughter': 306, 'stan': 307, \"isn't\": 308, 'mad': 309, 'after': 310, 'die': 311, 'ow': 312, \"someone's\": 313, 'barrel': 314, 'run': 315, 'bill': 316, 'lead': 317, \"playin'\": 318, 'dead': 319, \"lil'\": 320, 'whack': 321, 'true': 322, 'whole': 323, 'video': 324, 'wake': 325, \"they'll\": 326, 'motherfucker': 327, 'mine': 328, \"nothin'\": 329, \"bein'\": 330, 'prick': 331, \"usin'\": 332, 'sleep': 333, 'b': 334, 'dick': 335, 'channel': 336, \"mothafuckin'\": 337, 'mumble': 338, 'kim': 339, 'live': 340, 'around': 341, 'better': 342, 'white': 343, 'thinks': 344, 'over': 345, 'write': 346, 'right': 347, 'fight': 348, 'kells': 349, 'attempt': 350, 'skills': 351, 'simpson': 352, 'sing': 353, 'walked': 354, 'adore': 355, 'star': 356, 'player': 357, 'ones': 358, 'demand': 359, 'hand': 360, 'does': 361, 'massive': 362, 'asked': 363, 'offend': 364, 'men': 365, 'two': 366, 'other': 367, 'pay': 368, 'sit': 369, 'try': 370, 'bad': 371, 'rest': 372, 'has': 373, 'there': 374, 'mess': 375, 'help': 376, 'amy': 377, 'cake': 378, 'saw': 379, 'looked': 380, 'going': 381, 'dre': 382, \"something's\": 383, 'wrong': 384, 'feeling': 385, 'means': 386, 'trouble': 387, 'slap': 388, 'box': 389, 'must': 390, 'ever': 391, 'hell': 392, 'kill': 393, 'wanna': 394, 'yap': 395, 'could': 396, 'bombs': 397, 'rappers': 398, 'show': 399, 'use': 400, 'rhyme': 401, 'full': 402, 'hall': 403, 'church': 404, 'flames': 405, 'ha': 406, 'every': 407, 'nobody': 408, 'nascar': 409, 'asgard': 410, 'maybe': 411, 'hungry': 412, \"comin'\": 413, 'normal': 414, 'being': 415, 'ray': 416, 'fab': 417, 'radio': 418, 'lumma': 419, 'human': 420, 'anything': 421, 'lose': 422, 'songs': 423, 'verses': 424, 'myself': 425, 'satan': 426, 'seat': 427, \"there's\": 428, \"they're\": 429, 'mistake': 430, 'moves': 431, \"she's\": 432, 'treat': 433, 'ching': 434, 'psych': 435, 'mary': 436, 'scene': 437, \"shit's\": 438, 'party': 439, 'start': 440, 'bloody': 441, 'pen': 442, 'addiction': 443, 'magician': 444, 'critics': 445, 'crickets': 446, 'fence': 447, 'whether': 448, 'picket': 449, 'impaled': 450, 'stick': 451, 'pale': 452, 'pigment': 453, 'ham': 454, 'shout': 455, 'kendrick': 456, 'bring': 457, 'vintage': 458, 'art': 459, 'mcing': 460, 'mixed': 461, 'vinci': 462, \"stimpy's\": 463, 'public': 464, 'enemy': 465, 'thought': 466, 'pe': 467, 'gym': 468, 'house': 469, 'until': 470, 'volume': 471, 'loud': 472, 'mayhem': 473, 'bucket': 474, 'short': 475, 'body': 476, 'dressed': 477, \"khaki's\": 478, 'pressed': 479, 'nike': 480, 'crispy': 481, 'fresh': 482, 'laced': 483, 'aftershave': 484, 'cologne': 485, 'faint': 486, 'plus': 487, 'showed': 488, 'coat': 489, 'fresher': 490, 'wet': 491, 'paint': 492, 'chess': 493, 'check': 494, \"body's\": 495, 'banging': 496, 'jump': 497, 'bang': 498, 'siree': 499, \"'bob'\": 500, 'thinking': 501, 'thang': 502, \"kid's\": 503, 'blowing': 504, 'valve': 505, 'slowing': 506, 'towel': 507, 'dumb': 508, 'question': 509, 'bozos': 510, 'smart': 511, 'stupid': 512, 'hope': 513, 'hoe': 514, 'powerful': 515, 'cough': 516, 'syrup': 517, 'styrofoam': 518, 'asleep': 519, 'monte': 520, 'carlo': 521, 'ugly': 522, 'kardashian': 523, 'lamar': 524, 'both': 525, 'set': 526, 'low': 527, 'far': 528, 'drugs': 529, 'past': 530, 'codeine': 531, 'tomorrow': 532, 'borrow': 533, 'trying': 534, 'find': 535, 'alone': 536, 'note': 537, 'potty': 538, 'soap': 539, 'lathered': 540, 'kangols': 541, 'less': 542, 'cargos': 543, 'fixing': 544, 'absurd': 545, \"ma'am\": 546, 'birdbrain': 547, 'called': 548, 'anybody': 549, 'birdman': 550, 'swallow': 551, 'rick': 552, 'heard': 553, 'discouraged': 554, 'toe': 555, 'sound': 556, 'shut': 557, 'become': 558, 'alright': 559, \"doin'\": 560, 'rihanna': 561, 'text': 562, 'left': 563, 'hickeys': 564, 'neck': 565, 'dissed': 566, 'perplexed': 567, 'insult': 568, 'compliment': 569, \"watchin'\": 570, '8': 571, 'nordictrack': 572, 'forgot': 573, 'autograph': 574, 'wrote': 575, 'starter': 576, 'cap': 577, 'son': 578, 'listen': 579, 'dad': 580, 'bun': 581, \"giant's\": 582, 'eyes': 583, 'open': 584, 'undeniable': 585, \"supplyin'\": 586, 'smoke': 587, 'fire': 588, 'stoked': 589, 'scope': 590, 'grazed': 591, 'interscope': 592, 'swayze': 593, 'reply': 594, 'crowd': 595, 'yelling': 596, 'petty': 597, 'corny': 598, 'lines': 599, 'ooh': 600, '45': 601, 'outselling': 602, '29': 603, 'blew': 604, \"somethin'\": 605, \"daughter's\": 606, \"stealin'\": 607, 'food': 608, 'mole': 609, 'hill': 610, 'mountain': 611, 'chill': 612, \"actin'\": 613, 'chrome': 614, 'bone': 615, 'marrow': 616, 'gunner': 617, 'bow': 618, 'arrow': 619, 'phone': 620, \"sprayin'\": 621, 'hold': 622, 'eating': 623, 'cereal': 624, 'oatmeal': 625, \"fuck's\": 626, 'bowl': 627, 'milk': 628, 'wheaties': 629, 'cheerios': 630, \"takin'\": 631, 'reading': 632, 'material': 633, 'dictionary': 634, 'four': 635, 'sucked': 636, 'recovery': 637, 'ago': 638, 'oops': 639, 'facts': 640, 'goof': 641, 'luxury': 642, \"'02\": 643, 'burn': 644, 'younger': 645, 'funny': 646, \"i'd\": 647, 'rather': 648, '80': 649, '20': 650, 'hitting': 651, 'age': 652, 'fill': 653, 'page': 654, '10': 655, \"old's\": 656, 'city': 657, 'kiddy': 658, 'play': 659, 'babysitting': 660, 'lil': 661, 'tay': 662, 'okay': 663, 'spent': 664, \"shootin'\": 665, 'dig': 666, 'grave': 667, 'billy': 668, 'goat': 669, 'list': 670, 'biggie': 671, 'jay': 672, 'taylor': 673, 'swift': 674, 'iggy': 675, 'putting': 676, 'ja': 677, 'benzino': 678, \"sayin'\": 679, 'hailey': 680, 'vain': 681, 'alien': 682, 'brain': 683, 'satanist': 684, 'biggest': 685, 'flops': 686, 'greatest': 687, 'hits': 688, \"game's\": 689, 'changed': 690, 'locks': 691, 'slay': 692, 'mwah': 693, 'jade': 694, 'kiss': 695, 'labor': 696, 'rich': 697, 'shamed': 698, 'clickbait': 699, 'state': 700, 'bliss': 701, 'goddamn': 702, 'aim': 703, 'champagne': 704, 'moment': 705, 'enjoy': 706, 'career': 707, 'destroy': 708, 'lethal': 709, 'injection': 710, 'feet': 711, 'effort': 712, 'foot': 713, '11': 714, \"you'd\": 715, 'record': 716, 'would': 717, 'suck': 718, 'second': 719, 'lick': 720, 'ballsack': 721, 'life': 722, 'solidified': 723, 'rambo': 724, 'bullets': 725, 'machine': 726, 'ammo': 727, 'tatted': 728, 'rapper': 729, 'battle': 730, \"he'll\": 731, 'flannel': 732, 'sandals': 733, 'knows': 734, \"gon'\": 735, 'shadow': 736, 'exhausting': 737, 'letting': 738, 'offspring': 739, 'dance': 740, 'sombrero': 741, 'salty': 742, 'young': 743, \"gerald's\": 744, 'balls': 745, 'inside': 746, 'halsey': 747, 'red': 748, 'sweater': 749, 'black': 750, 'leather': 751, 'dress': 752, 'death': 753, 'threat': 754, 'letter': 755, 'toothpick': 756, 'pic': 757, 'thanks': 758, 'dissing': 759, 'excuse': 760, 'alike': 761, 'care': 762, \"who's\": 763, \"losin'\": 764, 'picked': 765, 'else': 766, 'fails': 767, 'budden': 768, \"l's\": 769, 'nails': 770, 'coffins': 771, 'soft': 772, 'cottonelle': 773, 'killshot': 774, 'fail': 775, \"idiot's\": 776, 'boss': 777, 'pops': 778, 'pills': 779, 'tells': 780, \"hit's\": 781, 'admits': 782, 'pac': 783, 'killed': 784, 'ah': 785, 'auto': 786, 'tune': 787, 'blonde': 788, 'earrings': 789, 'mirror': 790, 'leave': 791, \"d'you\": 792, 'miss': 793, 'chorus': 794, 'ahem': 795, 'popular': 796, 'zantac': 797, 'antacid': 798, 'ready': 799, 'tackle': 800, 'task': 801, 'fantastic': 802, 'grand': 803, 'masses': 804, 'stands': 805, 'massacre': 806, \"kardashian's\": 807, 'stomped': 808, 'hands': 809, 'gluteus': 810, 'maximus': 811, 'squeeze': 812, 'squish': 813, 'pass': 814, 'nasty': 815, 'ask': 816, 'lesbian': 817, 'lindsay': 818, 'seeing': 819, \"samantha's\": 820, 'practically': 821, 'ten': 822, 'grin': 823, 'enforcer': 824, 'torture': 825, 'cutest': 826, 'charley': 827, 'horse': 828, 'portia': 829, \"what's\": 830, 'ellen': 831, 'degeneres': 832, 'telling': 833, 'tenderness': 834, 'gentle': 835, 'smooth': 836, 'gentleman': 837, 'ventolin': 838, 'inhaler': 839, 'xenadrine': 840, 'invite': 841, 'sarah': 842, 'palin': 843, 'dinner': 844, 'nail': 845, 'hello': 846, 'brit': 847, 'cut': 848, 'middle': 849, 'end': 850, 'hospital': 851, \"won't\": 852, 'ritalin': 853, 'binge': 854, 'attention': 855, 'mention': 856, \"jennifer's\": 857, 'john': 858, 'mayer': 859, 'bench': 860, 'swear': 861, 'guys': 862, 'inch': 863, 'style': 864, 'without': 865, \"checkin'\": 866, 'deny': 867, 'beginning': 868, 'sprout': 869, 'alfalfa': 870, 'wash': 871, 'filthy': 872, 'hear': 873, 'album': 874, 'such': 875, 'finesse': 876, 'nostalgia': 877, 'cash': 878, 'alba': 879, 'breasts': 880, 'wowzers': 881, 'trousers': 882, 'wonder': 883, 'dressing': 884, 'elvis': 885, 'lord': 886, 'us': 887, 'pink': 888, 'alf': 889, 'shirt': 890, 'someone': 891, 'shrinked': 892, 'outfit': 893, 'flip': 894, 'assured': 895, \"superman's\": 896, 'rescue': 897, 'blake': 898, 'matter': 899, 'birthday': 900, 'blade': 901, 'jail': 902, 'met': 903, 'soul': 904, 'rehab': 905, 'haha': 906, 'dr': 907, '2020': 908, 'easy': 909, 'hurt': 910, 'feelings': 911, 'chance': 912, \"i've\": 913, 'happen': 914, 'bananas': 915, 'taking': 916, 'chances': 917, 'ordered': 918, 'their': 919, 'arms': 920, 'robot': 921, 'bot': 922, 'computer': 923, 'genes': 924, 'laptop': 925, 'pocket': 926, \"pen'll\": 927, 'fat': 928, 'knot': 929, 'profit': 930, \"livin'\": 931, \"killin'\": 932, 'clinton': 933, 'office': 934, 'monica': 935, 'lewinsky': 936, \"feelin'\": 937, 'nutsack': 938, 'honest': 939, 'rude': 940, 'indecent': 941, 'syllables': 942, 'skill': 943, 'holic': 944, 'flippity': 945, 'dippity': 946, 'hippity': 947, \"pissin'\": 948, 'match': 949, 'rappity': 950, 'brat': 951, \"packin'\": 952, 'mac': 953, \"ac'\": 954, 'backpack': 955, 'crap': 956, 'yackety': 957, 'yack': 958, 'exact': 959, 'lyrical': 960, 'acrobat': 961, 'stunts': 962, \"practicin'\": 963, 'able': 964, 'table': 965, 'couple': 966, 'faggots': 967, 'crack': 968, 'ironic': 969, 'signed': 970, 'aftermath': 971, 'drop': 972, 'f': 973, 'wrath': 974, \"havin'\": 975, 'rough': 976, 'period': 977, 'maxi': 978, 'actually': 979, 'disastrously': 980, 'masterfully': 981, 'constructing': 982, 'masterpièce': 983, \"maintainin'\": 984, 'key': 985, 'secret': 986, 'immortality': 987, 'ι': 988, 'truthful': 989, \"blueprint's\": 990, 'simply': 991, 'youthful': 992, 'exuberance': 993, 'loves': 994, 'root': 995, 'nuisance': 996, 'earth': 997, 'asteroid': 998, 'nothing': 999, 'moon': 1000, 'pew': 1001, 'mcs': 1002, 'taken': 1003, 'vehicle': 1004, 'bus': 1005, 'students': 1006, 'product': 1007, 'rakim': 1008, 'lakim': 1009, 'shabazz': 1010, '2pac': 1011, 'n': 1012, 'w': 1013, 'cube': 1014, 'yella': 1015, 'eazy': 1016, 'thank': 1017, 'inspired': 1018, 'position': 1019, 'meet': 1020, 'c': 1021, 'induct': 1022, 'roll': 1023, 'burst': 1024, 'ball': 1025, 'inducted': 1026, 'alcohol': 1027, 'wall': 1028, 'shame': 1029, 'fags': 1030, 'flock': 1031, 'plank': 1032, \"thinkin'\": 1033, 'barely': 1034, \"witnessin'\": 1035, 'mass': 1036, 'occur': 1037, 'watching': 1038, 'gathering': 1039, 'place': 1040, 'oy': 1041, 'vey': 1042, \"boy's\": 1043, 'thumbs': 1044, 'pat': 1045, 'label': 1046, 'work': 1047, 'everything': 1048, 'outta': 1049, 'basically': 1050, 'capable': 1051, \"keepin'\": 1052, 'pace': 1053, \"racin'\": 1054, 'dale': 1055, 'earnhardt': 1056, 'trailer': 1057, 'park': 1058, 'trash': 1059, 'kneel': 1060, 'general': 1061, 'zod': 1062, \"planet's\": 1063, 'krypton': 1064, 'thor': 1065, 'odin': 1066, 'rodent': 1067, 'omnipotent': 1068, \"reloadin'\": 1069, 'immediately': 1070, \"totin'\": 1071, 'woken': 1072, \"walkin'\": 1073, \"talkin'\": 1074, 'zombie': 1075, \"floatin'\": 1076, 'mom': 1077, \"throatin'\": 1078, 'ramen': 1079, 'noodle': 1080, 'common': 1081, 'poodle': 1082, 'doberman': 1083, 'pinch': 1084, 'arm': 1085, 'homage': 1086, 'pupil': 1087, \"honesty's\": 1088, 'brutal': 1089, 'honestly': 1090, 'futile': 1091, 'utilize': 1092, 'sure': 1093, 'somewhere': 1094, 'chicken': 1095, 'scribble': 1096, 'doodle': 1097, 'tough': 1098, 'times': 1099, 'few': 1100, 'punchlines': 1101, 'case': 1102, 'unsigned': 1103, 'lunchtime': 1104, 'where': 1105, 'underground': 1106, 'pharoahe': 1107, 'monch': 1108, 'grind': 1109, 'crunch': 1110, 'sometimes': 1111, 'combine': 1112, 'appeal': 1113, 'skin': 1114, 'color': 1115, \"tryin'\": 1116, 'censor': 1117, 'lp': 1118, '1': 1119, 'tried': 1120, 'seven': 1121, 'kids': 1122, 'columbine': 1123, 'add': 1124, 'ak': 1125, '47': 1126, 'revolver': 1127, '9': 1128, \"morphin'\": 1129, 'immortal': 1130, 'portal': 1131, 'stuck': 1132, 'warp': 1133, '2004': 1134, 'pointless': 1135, 'rapunzel': 1136, 'cornrows': 1137, 'bought': 1138, 'raygun': 1139, 'fabolous': 1140, 'fag': 1141, \"mayweather's\": 1142, \"singin'\": 1143, 'played': 1144, 'piano': 1145, '24': 1146, '7': 1147, 'special': 1148, 'cable': 1149, 'went': 1150, 'station': 1151, 'very': 1152, 'lyrics': 1153, 'supersonic': 1154, 'speed': 1155, 'fad': 1156, 'uh': 1157, 'summa': 1158, 'dooma': 1159, \"assumin'\": 1160, 'superhuman': 1161, 'innovative': 1162, 'rubber': 1163, \"ricochetin'\": 1164, \"it'll\": 1165, 'glue': 1166, 'devastating': 1167, 'demonstrating': 1168, 'audience': 1169, 'levitating': 1170, 'fading': 1171, 'forever': 1172, 'waiting': 1173, 'celebrating': 1174, 'motivated': 1175, 'elevating': 1176, 'elevator': 1177, 'mainstream': 1178, 'jealous': 1179, 'confuse': 1180, 'found': 1181, 'hella': 1182, 'fuse': 1183, 'shock': 1184, 'words': 1185, 'occurs': 1186, \"rippin'\": 1187, 'versus': 1188, 'curtains': 1189, 'inadvertently': 1190, \"hurtin'\": 1191, 'many': 1192, 'murder': 1193, 'prove': 1194, 'were': 1195, 'nice': 1196, 'sacrifice': 1197, 'virgins': 1198, 'ugh': 1199, 'flunky': 1200, 'pill': 1201, 'junkie': 1202, 'accolades': 1203, 'brung': 1204, 'bully': 1205, 'mind': 1206, 'million': 1207, 'leagues': 1208, 'above': 1209, 'ill': 1210, 'speak': 1211, 'tongues': 1212, 'tongue': 1213, 'cheek': 1214, 'drunk': 1215, 'fucking': 1216, 'wheel': 1217, \"bumpin'\": 1218, 'heavy': 1219, 'boyz': 1220, 'chunky': 1221, 'funky': 1222, 'something': 1223, 'tugging': 1224, 'struggling': 1225, 'angels': 1226, 'devils': 1227, \"askin'\": 1228, 'eliminate': 1229, 'hate': 1230, 'consideration': 1231, 'bitter': 1232, 'hatred': 1233, 'may': 1234, 'patient': 1235, 'sympathetic': 1236, 'situation': 1237, 'understand': 1238, 'discrimination': 1239, \"handin'\": 1240, 'lemons': 1241, 'lemonade': 1242, 'batter': 1243, 'supposed': 1244, 'fatal': 1245, 'overseas': 1246, 'vacation': 1247, 'trip': 1248, 'broad': 1249, 'fall': 1250, 'retard': 1251, 'almost': 1252, 'belly': 1253, 'dancer': 1254, 'shaking': 1255, 'nelly': 1256, 'jams': 1257, 'answer': 1258, 'finished': 1259, 'peeing': 1260, 'resisting': 1261, 'arrest': 1262, 'agreeing': 1263, 'already': 1264, 'knees': 1265, 'ground': 1266, 'further': 1267, 'impossible': 1268, 'murderer': 1269, 'r': 1270, 'song': 1271, 'ring': 1272, 'forgive': 1273, 'dog': 1274, 'mere': 1275, 'tempting': 1276, 'nick': 1277, 'mtv': 1278, 'goes': 1279, 'kate': 1280, 'ashley': 1281, 'used': 1282, 'wholesome': 1283, 'getting': 1284, 'older': 1285, 'starting': 1286, 'bum': 1287, 'bums': 1288, 'movies': 1289, 'popcorn': 1290, 'geez': 1291, 'ticket': 1292, \"zipper's\": 1293, 'zipped': 1294, 'remove': 1295, 'movie': 1296, 'theatre': 1297, \"kate's\": 1298, 'shower': 1299, \"didn't\": 1300, 'obscene': 1301, 'great': 1302, 'wee': 1303, 'herman': 1304, \"movie's\": 1305, 'pg': 1306, 'attorney': 1307, 'simple': 1308, 'plead': 1309, 'innocent': 1310, 'cop': 1311, 'plea': 1312, 'streets': 1313, \"lawyer's\": 1314, 'michael': 1315, 'busy': 1316, 'britney': 1317, 'spears': 1318, 'shoulders': 1319, 'laugh': 1320, 'hahaha': 1321, 'hilary': 1322, 'duff': 1323, 'quite': 1324, 'butt': 1325, \"she'll\": 1326, 'dances': 1327, 'sings': 1328, 'bozo': 1329, 'boyfriend': 1330, 'hi': 1331, 'jojo': 1332, 'computers': 1333, 'seized': 1334, 'keys': 1335, 'ranch': 1336, 'cookies': 1337, 'lookie': 1338, 'whiff': 1339, 'jesus': 1340, 'juice': 1341, 'sip': 1342, 'safe': 1343, 'janet': 1344, 'breast': 1345, 'tit': 1346, 'working': 1347, 'flee': 1348, 'chopper': 1349, 'arnold': 1350, 'gwen': 1351, 'stefani': 1352}\n",
      "total words: 1353\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Generate the word index dictionary\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Define the total words. You add 1 for the index `0` which is just the padding token.\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f'word index dictionary: {tokenizer.word_index}')\n",
    "print(f'total words: {total_words}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# Initialize the sequences list\n",
    "input_sequences = []\n",
    "\n",
    "# Loop over every line\n",
    "for line in corpus:\n",
    "\n",
    "\t# Tokenize the current line\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "\t# Loop over the line several times to generate the subphrases\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\n",
    "\t\t# Generate the subphrase\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\n",
    "\t\t# Append the subphrase to the sequences list\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Get the length of the longest line\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad all sequences\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create inputs and label by splitting the last token in the subphrases\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "# Convert the label into one-hot arrays\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence: ['now', 'this', \"shit's\", 'about', 'to', 'kick', 'off', 'this', 'party', 'looks', 'wack']\n",
      "[56, 21, 438, 76, 5, 152, 68, 21, 439, 261, 262]\n"
     ]
    }
   ],
   "source": [
    "# Get sample sentence\n",
    "sentence = corpus[0].split()\n",
    "print(f'sample sentence: {sentence}')\n",
    "\n",
    "# Initialize token list\n",
    "token_list = []\n",
    "\n",
    "# Look up the indices of each word and append to the list\n",
    "for word in sentence:\n",
    "  token_list.append(tokenizer.word_index[word])\n",
    "\n",
    "# Print the token list\n",
    "print(token_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, 17, 100)           135300    \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirecti  (None, 40)               19360     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 40)                0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 728)               29848     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1353)              986337    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,170,845\n",
      "Trainable params: 1,170,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "lstm_units = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "          Embedding(total_words, embedding_dim, input_length=max_sequence_len-1),\n",
    "          Bidirectional(LSTM(lstm_units)),\n",
    "\t\t  Dropout(0.4),\n",
    "\t\t  Dense(728, activation='relu'),\n",
    "          Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "# Use categorical crossentropy because this is a multi-class problem\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 03:01:27.845342: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-13 03:01:28.320124: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-13 03:01:28.331598: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-13 03:01:29.182004: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-13 03:01:29.202421: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 8s 36ms/step - loss: 6.5700 - accuracy: 0.0270\n",
      "Epoch 2/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 6.0946 - accuracy: 0.0320\n",
      "Epoch 3/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 5.9339 - accuracy: 0.0338\n",
      "Epoch 4/80\n",
      "124/124 [==============================] - 4s 34ms/step - loss: 5.7655 - accuracy: 0.0459\n",
      "Epoch 5/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 5.5412 - accuracy: 0.0545\n",
      "Epoch 6/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 5.3177 - accuracy: 0.0673\n",
      "Epoch 7/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 5.1166 - accuracy: 0.0794\n",
      "Epoch 8/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 4.9339 - accuracy: 0.0913\n",
      "Epoch 9/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 4.7432 - accuracy: 0.1100\n",
      "Epoch 10/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 4.5604 - accuracy: 0.1140\n",
      "Epoch 11/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 4.3986 - accuracy: 0.1266\n",
      "Epoch 12/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 4.2069 - accuracy: 0.1410\n",
      "Epoch 13/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 4.0311 - accuracy: 0.1554\n",
      "Epoch 14/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 3.8435 - accuracy: 0.1723\n",
      "Epoch 15/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 3.6698 - accuracy: 0.1927\n",
      "Epoch 16/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 3.4817 - accuracy: 0.2124\n",
      "Epoch 17/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 3.3020 - accuracy: 0.2333\n",
      "Epoch 18/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 3.1191 - accuracy: 0.2615\n",
      "Epoch 19/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 2.9802 - accuracy: 0.2850\n",
      "Epoch 20/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 2.8254 - accuracy: 0.3034\n",
      "Epoch 21/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 2.6729 - accuracy: 0.3322\n",
      "Epoch 22/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 2.5462 - accuracy: 0.3569\n",
      "Epoch 23/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 2.4304 - accuracy: 0.3816\n",
      "Epoch 24/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 2.3392 - accuracy: 0.3937\n",
      "Epoch 25/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 2.2208 - accuracy: 0.4212\n",
      "Epoch 26/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 2.1351 - accuracy: 0.4393\n",
      "Epoch 27/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 2.0409 - accuracy: 0.4711\n",
      "Epoch 28/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 1.9848 - accuracy: 0.4787\n",
      "Epoch 29/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 1.8552 - accuracy: 0.5039\n",
      "Epoch 30/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 1.8319 - accuracy: 0.5069\n",
      "Epoch 31/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 1.7697 - accuracy: 0.5213\n",
      "Epoch 32/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 1.6958 - accuracy: 0.5445\n",
      "Epoch 33/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 1.6225 - accuracy: 0.5544\n",
      "Epoch 34/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 1.5687 - accuracy: 0.5735\n",
      "Epoch 35/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 1.5193 - accuracy: 0.5791\n",
      "Epoch 36/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 1.4709 - accuracy: 0.5939\n",
      "Epoch 37/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 1.4389 - accuracy: 0.6055\n",
      "Epoch 38/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 1.3778 - accuracy: 0.6179\n",
      "Epoch 39/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 1.3468 - accuracy: 0.6219\n",
      "Epoch 40/80\n",
      "124/124 [==============================] - 5s 44ms/step - loss: 1.3111 - accuracy: 0.6300\n",
      "Epoch 41/80\n",
      "124/124 [==============================] - 4s 36ms/step - loss: 1.2837 - accuracy: 0.6474\n",
      "Epoch 42/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 1.2341 - accuracy: 0.6552\n",
      "Epoch 43/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 1.1978 - accuracy: 0.6658\n",
      "Epoch 44/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 1.1662 - accuracy: 0.6779\n",
      "Epoch 45/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 1.1290 - accuracy: 0.6769\n",
      "Epoch 46/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 1.1175 - accuracy: 0.6832\n",
      "Epoch 47/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 1.1177 - accuracy: 0.6807\n",
      "Epoch 48/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 1.0610 - accuracy: 0.6900\n",
      "Epoch 49/80\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 1.0514 - accuracy: 0.7077\n",
      "Epoch 50/80\n",
      "124/124 [==============================] - 4s 35ms/step - loss: 1.0177 - accuracy: 0.7029\n",
      "Epoch 51/80\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.9915 - accuracy: 0.7117\n",
      "Epoch 52/80\n",
      "124/124 [==============================] - 4s 34ms/step - loss: 0.9791 - accuracy: 0.7170\n",
      "Epoch 53/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 0.9390 - accuracy: 0.7223\n",
      "Epoch 54/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.9048 - accuracy: 0.7342\n",
      "Epoch 55/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.9062 - accuracy: 0.7415\n",
      "Epoch 56/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.9030 - accuracy: 0.7314\n",
      "Epoch 57/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.8581 - accuracy: 0.7491\n",
      "Epoch 58/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 0.8506 - accuracy: 0.7556\n",
      "Epoch 59/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.8303 - accuracy: 0.7506\n",
      "Epoch 60/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.8436 - accuracy: 0.7549\n",
      "Epoch 61/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.8242 - accuracy: 0.7513\n",
      "Epoch 62/80\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.8176 - accuracy: 0.7571\n",
      "Epoch 63/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.8181 - accuracy: 0.7483\n",
      "Epoch 64/80\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.7824 - accuracy: 0.7707\n",
      "Epoch 65/80\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.7590 - accuracy: 0.7680\n",
      "Epoch 66/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 0.7423 - accuracy: 0.7783\n",
      "Epoch 67/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 0.7455 - accuracy: 0.7803\n",
      "Epoch 68/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.7372 - accuracy: 0.7763\n",
      "Epoch 69/80\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 0.7088 - accuracy: 0.7945\n",
      "Epoch 70/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.7022 - accuracy: 0.7919\n",
      "Epoch 71/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 0.6787 - accuracy: 0.7965\n",
      "Epoch 72/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.6876 - accuracy: 0.7942\n",
      "Epoch 73/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.6715 - accuracy: 0.8003\n",
      "Epoch 74/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.6667 - accuracy: 0.8028\n",
      "Epoch 75/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.6517 - accuracy: 0.8038\n",
      "Epoch 76/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.6607 - accuracy: 0.8023\n",
      "Epoch 77/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.6406 - accuracy: 0.8013\n",
      "Epoch 78/80\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.6389 - accuracy: 0.8068\n",
      "Epoch 79/80\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.6099 - accuracy: 0.8164\n",
      "Epoch 80/80\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 0.6046 - accuracy: 0.8116\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(xs, ys, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 03:06:43.170959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-13 03:06:43.352711: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-13 03:06:43.368602: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girl i like you and you like me seen a ass like that i'm pee wee herman this movie's pg pupil sprayin' lead a rage and got slim but enough this music you make like that hard that hard that induct them that old's rage and pay them little attention yack stunts blew blew blew so make songs like my filthy mouth out my woo woo woo now break it was clear to me clear to me fuckin' the m m m m m m m mathers marshall mathers did that was so jennifer's in the rage and pay homage lyrical lyrical acrobat stunts while i was but\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Girl i like you and you like me\"\n",
    "# Define total words to predict\n",
    "next_words = 100\n",
    "\n",
    "\n",
    "for _ in range(next_words):\n",
    "\t# Convert the text into sequences\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\t# Pad the sequences\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t# Get the probabilities of predicting a word\n",
    "\tpredicted = model.predict(token_list, verbose=0)\n",
    "\t# Choose the next word based on the maximum probability\n",
    "\tpredicted = np.argmax(predicted, axis=-1).item()\n",
    "\t# Get the actual word from the word index\n",
    "\toutput_word = tokenizer.index_word[predicted]\n",
    "\t# Append to the current text\n",
    "\tseed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}